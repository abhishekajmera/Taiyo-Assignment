{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "import pandas as pd\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Harvester:\n",
    "    \n",
    "\n",
    "    def __init__(self):  \n",
    "        self.url_array=[] \n",
    "        self.final_array=[]\n",
    "        \n",
    "    #function to scrape the URLs of institutions from the first 40 pages of the site\n",
    "    def get_urls(self):\n",
    "       \n",
    "        \n",
    "        arr2=[]\n",
    "        #Xpath of the URL\n",
    "        xpath_string='//*[@id=\"page-top\"]/main/article/div[1]/div/div[2]/div/div[1]/div/div[9]/div/h4/a';\n",
    "        \n",
    "        #Starting the driver\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get('https://www.grid.ac/institutes') \n",
    "        \n",
    "        #Looping over the first 40 pages\n",
    "        for j in range(40):\n",
    "            \n",
    "            xpath_string='//*[@id=\"page-top\"]/main/article/div[1]/div/div[2]/div/div[1]/div/div[9]/div/h4/a';\n",
    "            #Modifying the xpath string and fetching the URLs for the first 8 institutions\n",
    "            for x in range(2,10):\n",
    "                xpath_string=xpath_string[0:70]+str(x)+xpath_string[71:]\n",
    "                elem=driver.find_elements_by_xpath(xpath_string)[0]\n",
    "                href=elem.get_attribute('href')\n",
    "                self.url_array.append(href)\n",
    "             \n",
    "            #Modifying the xpath string and fetching the URLs for the rest of the institutions\n",
    "            xpath_string='//*[@id=\"page-top\"]/main/article/div[1]/div/div[2]/div/div[1]/div/div[10]/div/h4/a';\n",
    "            for x in range(10,27):\n",
    "                xpath_string=xpath_string[0:70]+str(x)+xpath_string[72:]\n",
    "                elem=driver.find_elements_by_xpath(xpath_string)[0]\n",
    "                href=elem.get_attribute('href')\n",
    "                self.url_array.append(href)\n",
    "            \n",
    "            #Clicking the 'Next' button and going to the next page    \n",
    "            submit_button = driver.find_elements_by_xpath('//*[@id=\"page-top\"]/main/article/div[1]/div/div[2]/div/div[2]/nav/span[10]/a')[0]\n",
    "            submit_button.click()\n",
    "            time.sleep(4)\n",
    "\n",
    "              \n",
    "        self.parse_urls()\n",
    "        return self.final_array\n",
    "            \n",
    "        \n",
    "        #Function to scrape parameters from link passed to it\n",
    "    def get_params(self,link):\n",
    "            \n",
    "        #Initializing dictionary\n",
    "        Params={\n",
    "            'Institute_name':\"\",\n",
    "            'Grid_id':\"\",\n",
    "            'Type':\"\",\n",
    "            'Established_year':\"\",\n",
    "            'Institute_link':\"\",\n",
    "            'Wikipedia_link':\"\",\n",
    "            'Alias_list':\"\",\n",
    "            'City':\"\",\n",
    "            'City_geoNames_Code':\"\",\n",
    "            'City_geoNames_id':\"\",\n",
    "            'Country':\"\",\n",
    "            'Country_geoNames_Code':\"\",\n",
    "            'Country_geoNames_id':\"\",\n",
    "            'NUTS1_name':\"\",\n",
    "            'NUTS1_code':\"\",\n",
    "            'NUTS2_name':\"\",\n",
    "            'NUTS2_code':\"\",\n",
    "            'NUTS3_name':\"\",\n",
    "            'NUTS3_code':\"\"\n",
    "            }\n",
    "\n",
    "\n",
    "        time.sleep(3)\n",
    "        \n",
    "        #Getting HTML from the webpage\n",
    "        html_text=requests.get(link).text\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        dom = etree.HTML(str(soup))\n",
    "        \n",
    "        #Scraping parameters\n",
    "        try:\n",
    "            Params['Institute_name']=dom.xpath('//*[@id=\"page-top\"]/header/div/h1')[0].text\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            Params['Grid_id']=dom.xpath('//*[@id=\"institute-grid-id\"]')[0].text\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            Params['Type']=dom.xpath('//*[@id=\"page-top\"]/main/article/div/div[1]/div[1]/div[1]/div/div/div[2]/div/ul/li')[0].text.replace(\"\\n\",\"\").strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            Params['Established_year']=dom.xpath('//*[@id=\"page-top\"]/main/article/div/div[1]/div[1]/div[1]/div/div/div[3]/div/span')[0].text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            Params['Wikipedia_link']=dom.xpath('//*[@id=\"page-top\"]/main/article/div/div[1]/div[1]/div[2]/div/div/div[2]/div/a')[0].text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            Params['Institute_link']=dom.xpath('//*[@id=\"page-top\"]/main/article/div/div[1]/div[1]/div[2]/div/div/div[1]/div/ul/li/a')[0].text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            alias=soup.find_all(\"li\",itemprop=\"alternateName\")\n",
    "            alias_list=\"\"\n",
    "            for alt in alias:\n",
    "                alias_list= alias_list+ alt.text.strip()+\",\"\n",
    "            Params['Alias_list']=alias_list\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            Params['City']=soup.find(text=\"City\").find_next('td').contents[0]\n",
    "            Params['City_geoNames_Code']=soup.find(text=\"City\").find_next('td').find_next('td').text.strip()\n",
    "            Params['City_geoNames_id']=soup.find(text=City).find_next('td').find_next('td').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            Params['NUTS1_name']=soup.find(text=\"NUTS 1\").find_next('td').contents[0]\n",
    "            Params['NUTS1_code']=soup.find(text=\"NUTS 1\").find_next('td').find_next('td').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            Params['NUTS2_name']=soup.find(text=\"NUTS 2\").find_next('td').contents[0]\n",
    "            Params['NUTS2_code']=soup.find(text=\"NUTS 2\").find_next('td').find_next('td').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            Params['NUTS3_name']=soup.find(text=\"NUTS 3\").find_next('td').contents[0]\n",
    "            Params['NUTS3_code']=soup.find(text=\"NUTS 3\").find_next('td').find_next('td').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            Params['Country']=soup.find(text=\"Country/Territory\").find_next('td').contents[0]\n",
    "            Params['Country_geoNames_Code']=soup.find(text=\"Country/Territory\").find_next('td').find_next('td').text.strip()\n",
    "            Params['Country_geoNames_id']=soup.find(text=Country).find_next('td').find_next('td').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        #Appending dictionary to array\n",
    "        self.final_array.append(Params)\n",
    "\n",
    "    #Looping over each URL and fetching paramters   \n",
    "    def parse_urls(self):\n",
    "        for i in self.url_array:\n",
    "            self.get_params(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p= Data_Harvester()\n",
    "arr=p.get_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to dataframe\n",
    "df = pd.DataFrame(arr)\n",
    "\n",
    "#To scv\n",
    "df.to_csv('Institute_data.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Institute_name</th>\n",
       "      <th>Grid_id</th>\n",
       "      <th>Type</th>\n",
       "      <th>Established_year</th>\n",
       "      <th>Institute_link</th>\n",
       "      <th>Wikipedia_link</th>\n",
       "      <th>Alias_list</th>\n",
       "      <th>City</th>\n",
       "      <th>City_geoNames_Code</th>\n",
       "      <th>City_geoNames_id</th>\n",
       "      <th>Country</th>\n",
       "      <th>Country_geoNames_Code</th>\n",
       "      <th>Country_geoNames_id</th>\n",
       "      <th>NUTS1_name</th>\n",
       "      <th>NUTS1_code</th>\n",
       "      <th>NUTS2_name</th>\n",
       "      <th>NUTS2_code</th>\n",
       "      <th>NUTS3_name</th>\n",
       "      <th>NUTS3_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>National Natural Science Foundation of China</td>\n",
       "      <td>grid.419696.5</td>\n",
       "      <td>Government</td>\n",
       "      <td>1986 CE</td>\n",
       "      <td>http://www.nsfc.gov.cn/publish/portal1/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/National_Natural...</td>\n",
       "      <td>NSFC,</td>\n",
       "      <td>Beijing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>China</td>\n",
       "      <td>CN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>University of Toronto</td>\n",
       "      <td>grid.17063.33</td>\n",
       "      <td>Education</td>\n",
       "      <td>1827 CE</td>\n",
       "      <td>http://www.utoronto.ca/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/University_of_To...</td>\n",
       "      <td></td>\n",
       "      <td>Toronto</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Canada</td>\n",
       "      <td>CA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samsung (South Korea)</td>\n",
       "      <td>grid.419666.a</td>\n",
       "      <td>Company</td>\n",
       "      <td>1938 CE</td>\n",
       "      <td>http://www.samsung.com/sec/home/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Samsung</td>\n",
       "      <td></td>\n",
       "      <td>Seoul</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>South Korea</td>\n",
       "      <td>KR</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ministry of Science and Technology of the Peop...</td>\n",
       "      <td>grid.424020.0</td>\n",
       "      <td>Government</td>\n",
       "      <td>1998 CE</td>\n",
       "      <td>http://www.most.gov.cn/eng/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ministry_of_Scie...</td>\n",
       "      <td>State Science and Technology Commission,MOST,</td>\n",
       "      <td>Beijing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>China</td>\n",
       "      <td>CN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States Department of the Navy</td>\n",
       "      <td>grid.420434.5</td>\n",
       "      <td>Government</td>\n",
       "      <td>1798 CE</td>\n",
       "      <td>http://www.navy.mil/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/United_States_De...</td>\n",
       "      <td>DON,</td>\n",
       "      <td>Arlington</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Institute_name        Grid_id  \\\n",
       "0       National Natural Science Foundation of China  grid.419696.5   \n",
       "1                              University of Toronto  grid.17063.33   \n",
       "2                              Samsung (South Korea)  grid.419666.a   \n",
       "3  Ministry of Science and Technology of the Peop...  grid.424020.0   \n",
       "4               United States Department of the Navy  grid.420434.5   \n",
       "\n",
       "         Type Established_year                           Institute_link  \\\n",
       "0  Government          1986 CE  http://www.nsfc.gov.cn/publish/portal1/   \n",
       "1   Education          1827 CE                  http://www.utoronto.ca/   \n",
       "2     Company          1938 CE         http://www.samsung.com/sec/home/   \n",
       "3  Government          1998 CE              http://www.most.gov.cn/eng/   \n",
       "4  Government          1798 CE                     http://www.navy.mil/   \n",
       "\n",
       "                                      Wikipedia_link  \\\n",
       "0  https://en.wikipedia.org/wiki/National_Natural...   \n",
       "1  https://en.wikipedia.org/wiki/University_of_To...   \n",
       "2              https://en.wikipedia.org/wiki/Samsung   \n",
       "3  https://en.wikipedia.org/wiki/Ministry_of_Scie...   \n",
       "4  https://en.wikipedia.org/wiki/United_States_De...   \n",
       "\n",
       "                                      Alias_list       City  \\\n",
       "0                                          NSFC,    Beijing   \n",
       "1                                                   Toronto   \n",
       "2                                                     Seoul   \n",
       "3  State Science and Technology Commission,MOST,    Beijing   \n",
       "4                                           DON,  Arlington   \n",
       "\n",
       "  City_geoNames_Code City_geoNames_id        Country Country_geoNames_Code  \\\n",
       "0                                              China                    CN   \n",
       "1                                             Canada                    CA   \n",
       "2                                        South Korea                    KR   \n",
       "3                                              China                    CN   \n",
       "4                                      United States                    US   \n",
       "\n",
       "  Country_geoNames_id NUTS1_name NUTS1_code NUTS2_name NUTS2_code NUTS3_name  \\\n",
       "0                                                                              \n",
       "1                                                                              \n",
       "2                                                                              \n",
       "3                                                                              \n",
       "4                                                                              \n",
       "\n",
       "  NUTS3_code  \n",
       "0             \n",
       "1             \n",
       "2             \n",
       "3             \n",
       "4             "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
